{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-24T13:00:47.980707Z","iopub.execute_input":"2025-07-24T13:00:47.981054Z","iopub.status.idle":"2025-07-24T13:00:47.986233Z","shell.execute_reply.started":"2025-07-24T13:00:47.981030Z","shell.execute_reply":"2025-07-24T13:00:47.984968Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T13:00:48.007802Z","iopub.execute_input":"2025-07-24T13:00:48.008359Z","iopub.status.idle":"2025-07-24T13:00:50.117633Z","shell.execute_reply.started":"2025-07-24T13:00:48.008324Z","shell.execute_reply":"2025-07-24T13:00:50.116675Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# GLOBALS\ne = np.e\n\n# HYPERPARAMETERS\nepochs = 300\nalpha = .2 # learning rate\n\ndata = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ndata = np.array(data)\nrows, features = data.shape # saves rows and columns of dataset\nnp.random.shuffle(data)\n\ndata_dev = data[0:1000].T # data used for validation; the first 1k from MNST, transposed into columns\nY_dev = data_dev[0] # Y (ground truth) is the new first row of transposed subset, which contains all labels of the subset\nX_dev = data_dev[1:features] # everything that's not a label (so pixel values in this case)\nX_dev = X_dev / 255. #normalize image\n\n#data_train = data[200:rows].T # training dataset\ndata_train = data[1000:rows].T # training dataset\nY_train = data_train[0] # all labels of train\nX_train = data_train[1:features] # all non-labels, each row is a pixel location, each column is one image\nX_train = X_train / 255. #normalize image\n\ndef make_weights(a, b): \n    return np.random.rand(a, b) * 0.01, np.zeros(b)\n\ndef initial_params():\n    # inputs -> hidden layer 1\n    W1 = np.random.rand(10, 784) - 0.5\n    b1 = np.random.rand(10, 1) - 0.5 # initial weights = random between -.5 and .5, and u make 10x784 matrix\n    #cus of the amount of pixels in the first image. for this example youd do 100 actually, but 784 for\n    # the mnit stuff\n    # hidden layer 2\n    W2 = np.random.rand(10, 10) - 0.5\n    b2 = np.random.rand(10, 1) - 0.5\n    # fully connected layer 3, aka output layer\n    W3 = np.random.rand(10, 10) - 0.5\n    b3 = np.random.rand(10, 1) - 0.5\n    return W1, b1, W2, b2, W3, b3\n\ndef forward_prop(W1, b1, W2, b2, W3, b3, X):\n    Z1 = W1.dot(X) + b1 # for each individual Z_n = W_n*A_(n-1) + b_n, and this is like the matrix/\n    # vectors/fancy linear algebra version of doing that (instead of loops\n    # bc computers <3 matrices or whatever)\n    \n    # this is the first forward propagation, so X (the flattened image vector) represents the A_(n-1),\n    # aka activations of the first layer\n    \n    A1 = ReLU(Z1) # Z calculation passed through neuron activation function. first activation/full estimate\n\n    Z2 = W2.dot(A1) + b2 # second layer's calculation w/ weights and biases, based on A1 estimate. \n    A2 = ReLU(Z2) # activations of the 2nd layer's weights and biases calculations\n\n    Z3 = W3.dot(A2) + b3 # thrd layer's calculation w/ weights and biases, based on A2 estimate. \n    A3 = softmax(Z3) # activations of the 3rd layer's weights and biases calculations\n\n    return Z1, A1, Z2, A2, Z3, A3 # give back the activation values (A) and the stuff resulting from weights and biases (Z)\n    \n    \ndef Z (a_prev: float, w: float, b: float):\n    # given previous value, weight, and bias, returns z estimate\n    return np.dot(a_prev,w)+b\n    \ndef ReLU(z):\n    # FUNC: activation/deactivation of neurons by setting them to\n    # if z > 0:\n    #     return z\n    # else:\n    #     return 0\n    # lil graveyard for when we thought we would use loops insted of matrices o7\n    return np.maximum(0, z) # returns np array w zero if z val <= 0, and z otherwise\n\ndef deriv_relu(z):\n    # bro this is so dumb idek but like whatever\n    # if z > 0:\n    #     return 1\n    # else:\n    #     return 0\n    # relu = ReLU(z) # relu func\n    # relu[relu > 0] = 1 # sets all relu'd zs > 0 to 1, and keeps the rest \n    # return relu\n    return z > 0  \n\n# def softmax(Z):\n#     Z_probs = np.exp(Z) / sum(np.exp(Z)) \n#     return Z_probs\n# def softmax(Z):\n#     expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))  # numerical stability\n#     return expZ / np.sum(expZ, axis=0, keepdims=True)\n\ndef softmax(Z):\n    # Z is (10, m)\n    Z_exp = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n    return Z_exp / np.sum(Z_exp, axis=0, keepdims=True)\n\n# backwards propagation: estimates -> error -> how much weights/biases contributed to error -> adjustments\n    \ndef backpropagation(Z1, A1, Z2, A2, Z3, A3, W2, W3, X, Y):\n    m = X.shape[1]\n    # goes backwards through layers to find their contribution to loss\n\n    one_hot_Y = one_hot(Y) # get the one hot stuff for Y; so the [0 0 1]-esque shaped thing, representing what the\n    # final probabilities ideally should be, if the nn was fully accurate for this example, in order to compare it\n    \n    dZ3 = A3 - one_hot_Y # how wrong the estimatinos are\n    dW3 = 1/m * dZ3 @ A2.T \n    # you can derive thisss using skibidi rizz magci hahahahahahahahahahahahahahah\n    # sorry, dunno what happened, i meant to say that the avg change in loss in layer 3 with respect to weight\n    # = the average of the matrixly multiplied (??) gradients of the loss w/ respect to weight. that was very circular\n    # the weight comes from the W within the Z because like W is in the Z. im getting tired\n    db3 = 1/m * np.sum(dZ3) # the average of the absolute error\n    # not mean squared error btw\n\n    # holy cow FINALLY WE'RE ON LAYER 2. chris u have so much faith in us. idk if its desrverd. but ty ig\n    dZ2 = W3.T @ dZ3 * deriv_relu(Z2)\n    dW2 = 1/m * dZ2 @ A1.T\n    db2 = 1/m * np.sum(dZ2)\n\n    # layer 1!!!!!! youve seen the annotations\n    dZ1 = W2.T @ dZ2 * deriv_relu(Z1)\n    dW1 = 1/m * dZ1 @ X.T\n    # rhis time, its using X.T, because that's what A0 is\n    db1 = 1/m * np.sum(dZ1)\n\n    return dW3, db3, dW2, db2, dW1, db1 # returns all partial derivatives of weights and biases\n\ndef update_parameters(W1, b1, W2, b2, W3, b3, dW3, db3, dW2, db2, dW1, db1, alpha):\n    # FUNC: updates weights and biases based off of their contributions to loss and the learning rate (alpha)\n    W1 = W1 - alpha * dW1\n    W2 = W2 - alpha * dW2\n    W3 = W3 - alpha * dW3\n\n    b1 = b1 - alpha * db1\n    b2 = b2 - alpha * db2\n    b3 = b3 - alpha * db3\n    \n    return W1, b1, W2, b2, W3, b3\n\ndef gradient_descent(X, Y, iterations, alpha):\n    W1, b1, W2, b2, W3, b3 = initial_params()\n\n    for i in range (iterations): # runs forward prop, back prop, update params for # iterations told\n        Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X) \n        dW3, db3, dW2, db2, dW1, db1 = backpropagation(Z1, A1, Z2, A2, Z3, A3, W2, W3, X, Y)\n        W1, b1, W2, b2, W3, b3 = update_parameters(W1, b1, W2, b2, W3, b3, dW3, db3, dW2, db2, dW1, db1, alpha)        \n        #see function comments\n    \n        if i % 10 == 0: # prints stats for every tenth iteration\n            print(f\"Epoch: {i}\") # prints iteration number! rahh\n            print(f\"Accuracy: {get_accuracy(get_predictions(A3), Y)}\") # prints accuracy\n    print(f\"Total Epochs: {i}\") # prints iteration number! rahh\n    print(f\"Final Accuracy: {get_accuracy(get_predictions(A3), Y)}\") # prints accuracy\n    return W1, b1, W2, b2, W3, b3\n\ndef get_predictions(A3):\n    return np.argmax(A3, 0) # get biggest final activation, if theres nothing just say 0\n\ndef get_accuracy(predictions, Y):\n    # get accuracy of predictions\n    return np.sum(predictions == Y) / Y.size # how oftem predictions are right divded by the amt samples tested\n\ndef get_losses(estimates: np.array, ground_truths: np.array):\n    losses = []\n\n    for estimate, truth, in zip(estimates, ground_truths):\n        losses.append(estimate - truth)\n        \n    return losses\n\ndef one_hot(Y):\n    # make a matrix where each row is 0 if it is not the label number, and 1 if it is.\n    one_hot_y = np.zeros((Y.size, Y.max() + 1)) # empty numpy array of zeroes with a column for each digit possibility\n    # or 10, cuz (0, 1, 2, ..., 9)\n    one_hot_y[np.arange(Y.size), Y] = 1 # for each image/row (Y.size), set column corresponding to the digit value (index = Y) to 1 (true)\n    one_hot_y = one_hot_y.T # transposes; makes each column an example, instead of digit\n    return one_hot_y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T13:00:50.119194Z","iopub.execute_input":"2025-07-24T13:00:50.119481Z","iopub.status.idle":"2025-07-24T13:00:53.080784Z","shell.execute_reply.started":"2025-07-24T13:00:50.119459Z","shell.execute_reply":"2025-07-24T13:00:53.079968Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"W1, b1, W2, b2, W3, b3 = gradient_descent(X_train, Y_train, epochs, alpha)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T13:00:53.081768Z","iopub.execute_input":"2025-07-24T13:00:53.082120Z","iopub.status.idle":"2025-07-24T13:01:17.138578Z","shell.execute_reply.started":"2025-07-24T13:00:53.082094Z","shell.execute_reply":"2025-07-24T13:01:17.137575Z"}},"outputs":[{"name":"stdout","text":"Epoch: 0\nAccuracy: 0.09102439024390244\nEpoch: 10\nAccuracy: 0.15682926829268293\nEpoch: 20\nAccuracy: 0.2268048780487805\nEpoch: 30\nAccuracy: 0.36817073170731707\nEpoch: 40\nAccuracy: 0.4517317073170732\nEpoch: 50\nAccuracy: 0.44582926829268293\nEpoch: 60\nAccuracy: 0.5107317073170732\nEpoch: 70\nAccuracy: 0.5907317073170731\nEpoch: 80\nAccuracy: 0.6115121951219512\nEpoch: 90\nAccuracy: 0.6480243902439025\nEpoch: 100\nAccuracy: 0.6751707317073171\nEpoch: 110\nAccuracy: 0.6990243902439024\nEpoch: 120\nAccuracy: 0.7136341463414634\nEpoch: 130\nAccuracy: 0.7269512195121951\nEpoch: 140\nAccuracy: 0.7426585365853658\nEpoch: 150\nAccuracy: 0.7534146341463415\nEpoch: 160\nAccuracy: 0.763\nEpoch: 170\nAccuracy: 0.7716341463414634\nEpoch: 180\nAccuracy: 0.783\nEpoch: 190\nAccuracy: 0.7964634146341464\nEpoch: 200\nAccuracy: 0.7917804878048781\nEpoch: 210\nAccuracy: 0.8065365853658537\nEpoch: 220\nAccuracy: 0.8125121951219512\nEpoch: 230\nAccuracy: 0.8141219512195121\nEpoch: 240\nAccuracy: 0.8225121951219512\nEpoch: 250\nAccuracy: 0.8257317073170731\nEpoch: 260\nAccuracy: 0.8289024390243902\nEpoch: 270\nAccuracy: 0.8335365853658536\nEpoch: 280\nAccuracy: 0.8343170731707317\nEpoch: 290\nAccuracy: 0.8306829268292683\nTotal Epochs: 299\nFinal Accuracy: 0.8281219512195122\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(\"Test Accuracy:\")\nZ1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X_dev) #forward propagate through test dataset with finalized weights\npredictions = get_predictions(A3) # get predictions\nprint(\"Epoch: \", epochs, \"Accuracy: \", get_accuracy(predictions, Y_dev)) #print the accuracy of the predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T13:01:17.140658Z","iopub.execute_input":"2025-07-24T13:01:17.141010Z","iopub.status.idle":"2025-07-24T13:01:17.149288Z","shell.execute_reply.started":"2025-07-24T13:01:17.140985Z","shell.execute_reply":"2025-07-24T13:01:17.148133Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy:\nEpoch:  300 Accuracy:  0.807\n","output_type":"stream"}],"execution_count":14}]}